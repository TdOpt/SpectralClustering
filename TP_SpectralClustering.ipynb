{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spectral Clustering and Semi-Supervised Learning\n",
    "\n",
    "#### This notebook contains different values and imports that can be used in this practical session.\n",
    "#### Please keep the same variable names when provided in your report to make the work of teaching assistants easier. \n",
    "#### You can still change the values given or the sizes of the datasets treated if you believe it is usefull to illustrate your point.\n",
    "\n",
    "#### You can directly edit the markdown boxes to add your comments and answers to the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question 1 (imports and advised values):\n",
    "from sklearn.datasets import make_moons\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "matplotlib.style.use('seaborn-notebook')\n",
    "\n",
    "n_samples = 200 # You can change these values\n",
    "noise_level_list = [.05, .1, .2] # You can change these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A dictionnary which maps the noise level to the correponding moon dataset (if it has already been generated)\n",
    "noisy_moons = {} \n",
    "for lev in noise_level_list:\n",
    "    noisy_moons[lev] = make_moons(n_samples, shuffle = False, noise=lev)\n",
    "\n",
    "f, a = plt.subplots(1, len(noise_level_list))\n",
    "f.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "for i, lev in enumerate(noisy_moons):\n",
    "    a[i].scatter(noisy_moons[lev][0][:,0], noisy_moons[lev][0][:,1], c = noisy_moons[lev][1])\n",
    "    a[i].set_title('Noise = %.2f' % lev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2 :\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "n_neighbors_list = [1, 5, 10] # You can change these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_kneighbors_graph(axes, graph, pts, noise, k):\n",
    "    axes.scatter(pts[:, 0], pts[:, 1], marker='.')\n",
    "    xidx, yidx = graph.nonzero()\n",
    "    for i, j in zip(xidx, yidx):\n",
    "        if i < j:\n",
    "            axes.plot([pts[i, 0], pts[j, 0]], [pts[i, 1], pts[j, 1]], linewidth=0.5, c='g')\n",
    "    axes.set_title('Noise =  %.2f, k = %.1f' % (noise, k),fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate the adjacency matrices\n",
    "kNN_graphs = {}\n",
    "for k in n_neighbors_list:\n",
    "    for lev in noise_level_list:\n",
    "        kNN_graphs[k, lev] = kneighbors_graph(noisy_moons[lev][0], k, mode='distance', p=2)\n",
    "\n",
    "f, a = plt.subplots(len(n_neighbors_list), len(noise_level_list))\n",
    "f.subplots_adjust(wspace = 0.5, hspace=0.5)\n",
    "for i, k in enumerate(n_neighbors_list):\n",
    "    for j, lev in enumerate(noise_level_list):\n",
    "        plot_kneighbors_graph(a[i, j], kNN_graphs[k, lev], noisy_moons[lev][0], lev, k)\n",
    "        #a[i, j].imshow(kNN_graphs[k, lev].todense(), cmap = 'Greys', interpolation=\"none\")\n",
    "        #a[i, j].set_title('Noise =  %.2f, k = %.1f' % (lev, k),fontsize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 3 : Optimization problem \n",
    "\n",
    "- Given a graph $\\mathcal{G} = (E,V)$, let $D$ be its degree matrix and $W$ its weighted ajacency matrix, such that $L := D - W$ is the Laplacian of the graph. In the case when we consider a partition in 2 subsets $A,\\bar{A}$, the Normalized Cut problem translates as the following optimization problem:\n",
    "\n",
    "\n",
    "\n",
    "- $\\min_A \\{ f^\\top L f \\} \\text{ s.t. } f_i = \n",
    "\\begin{cases}\n",
    "    \\sqrt{\\frac{\\text{Vol}(\\bar{A})}{\\text{Vol}(A)}},& \\text{if } v_i\\in A\\\\\n",
    "    \\sqrt{\\frac{\\text{Vol}(A)}{\\text{Vol}(\\bar{A})}},& \\text{if } v_i\\in \\bar{A}\n",
    "\\end{cases}, Df \\bot \\mathbb{1}, f^\\top D f = \\text{Vol}(V)$\n",
    "\n",
    "\n",
    "- This problem is then relaxed as $\\min_{f\\in \\mathbb{R}^n} f^\\top L f \\text{ s.t. } Df \\bot \\mathbb{1}, f^\\top D f = \\text{Vol}(V)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 4 : Complete the code in the box below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "f, a = plt.subplots(len(n_neighbors_list), len(noise_level_list))\n",
    "f.subplots_adjust(wspace = 0.5, hspace=0.5)\n",
    "labels = {}\n",
    "for i, k in enumerate(n_neighbors_list):\n",
    "    for j, l in enumerate(noise_level_list):\n",
    "        spectral = SpectralClustering(n_clusters=2, eigen_solver='arpack', affinity=\"precomputed\")\n",
    "        labels[k, l] = spectral.fit_predict(kneighbors_graph(noisy_moons[l][0], k, mode='distance', p=2).todense())\n",
    "        a[i, j].scatter(noisy_moons[l][0][:,0], noisy_moons[l][0][:,1], c = labels[k, l])\n",
    "        a[i, j].set_title('Noise =  %.2f, k = %.1f' % (l, k), fontsize = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that increasing the number improves the robustness of the clustering.\n",
    "However, since it makes the adjacency matrix denser, it also makes the complexity greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The Frobenius inner product for matrices\n",
    "def frobenius(A, B):\n",
    "    return np.sum(np.multiply(A,B))\n",
    "\n",
    "#Compute the matrix representation C of a given clustering\n",
    "#ie Cij = 1 iff xi and xj are clustered together and i != j\n",
    "def matrix_clustering(labels):\n",
    "    n = len(labels)\n",
    "    C = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if (labels[i] == labels[j] and i != j):\n",
    "                C[i,j] = 1\n",
    "    return C\n",
    "\n",
    "def cosine_similarity(C1, C2):\n",
    "    return frobenius(C1, C2) / (np.linalg.norm(C1, 'fro')*np.linalg.norm(C2, 'fro'))\n",
    "\n",
    "#Return the average cosine similarity between a reference clustering and bootstrapped clusterings over B runs\n",
    "def cluster_stability(X, algo, clusters=2, B=10, f=0.8):\n",
    "    n = X.shape[0]\n",
    "    ref = algo(X, clusters)\n",
    "    \n",
    "    avg_similarity = 0\n",
    "    \n",
    "    for i in range(B):\n",
    "        indices = np.random.choice(n, f * n, replace=False)\n",
    "        bootstrapped = X[indices, :]\n",
    "        c = algo(bootstrapped, clusters)\n",
    "        avg_similarity += cosine_similarity(matrix_clustering(c), matrix_clustering(ref[indices]))\n",
    "        \n",
    "    return avg_similarity / B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the stability as a function of the number of neighbors used for spectral clustering, on noisy and non-noisy versions of the moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise_free_stability = []\n",
    "noisy_stability = []\n",
    "n_neighbors_list = np.arange(1,11,2)\n",
    "for k in n_neighbors_list:\n",
    "    noisy_stability.append(\n",
    "        cluster_stability(noisy_moons[.2][0], \n",
    "            (lambda X, clusters: SpectralClustering(n_clusters=clusters, \n",
    "                            affinity=\"precomputed\").fit_predict(kneighbors_graph(X, k, mode='distance', p=2).todense())),\n",
    "            B=50, f=0.8))\n",
    "    noise_free_stability.append(\n",
    "        cluster_stability(noisy_moons[.05][0], \n",
    "            (lambda X, clusters: SpectralClustering(n_clusters=clusters, \n",
    "                            affinity=\"precomputed\").fit_predict(kneighbors_graph(X, k, mode='distance', p=2).todense())), \n",
    "            B=50, f=0.8))\n",
    "\n",
    "plt.plot(n_neighbors_list, noisy_stability, label = \"Noisy\")\n",
    "plt.plot(n_neighbors_list, noise_free_stability, label = \"Noise-free\")\n",
    "plt.title(\"Stability of Spectral Clustering as a function of the number of neighbors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "noise_free_stability = []\n",
    "noisy_stability = []\n",
    "n_neighbors_list = np.arange(1,11,2)\n",
    "\n",
    "for k in n_neighbors_list:\n",
    "    noisy_stability.append(\n",
    "        cluster_stability(noisy_moons[.2][0], \n",
    "            (lambda X, clusters: AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "            connectivity=kneighbors_graph(X, k, mode='distance', p=2).todense(), linkage='ward').fit_predict(X)),B=20, f=0.8))\n",
    "    noise_free_stability.append(\n",
    "        cluster_stability(noisy_moons[.05][0], \n",
    "            (lambda X, clusters: AgglomerativeClustering(n_clusters=2, affinity='euclidean', \n",
    "            connectivity=kneighbors_graph(X, k, mode='distance', p=2).todense(), linkage='ward').fit_predict(X)),B=20, f=0.8))\n",
    "\n",
    "plt.plot(n_neighbors_list, noisy_stability, label = \"Noisy\")\n",
    "plt.plot(n_neighbors_list, noise_free_stability, label = \"Noise-free\")\n",
    "plt.title(\"Stability of Agglomerative Clustering as a function of the number of neighbors\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "mnist_ = load_digits()\n",
    "# Randomly choose 50% of samples (to reduce computational time), while keeping target proportions\n",
    "idx, _ = next(StratifiedShuffleSplit(n_splits=10, train_size=0.5).split(mnist_.data, mnist_.target))\n",
    "mnist = {'data': mnist_.data[idx], 'target': mnist_.target[idx]}\n",
    "\n",
    "# Compute PCA on dataset to further visualisation\n",
    "pca = PCA(n_components=2)\n",
    "proj_mnist = pca.fit_transform(mnist['data'])\n",
    "\n",
    "n_neighbors_list = np.arange(1, 11, 2)\n",
    "n_clusters_list = np.arange(2, 11, 2)\n",
    "mnist_stability = {}\n",
    "\n",
    "for C in n_clusters_list:\n",
    "    mnist_stability[C] = []\n",
    "    \n",
    "    for k in n_neighbors_list:\n",
    "        f = plt.figure(C*len(n_neighbors_list)+k)\n",
    "        # Compute Spectral Clustering\n",
    "        spectral = SpectralClustering(n_clusters=C, eigen_solver='arpack', affinity='precomputed')\n",
    "        knn_graph = kneighbors_graph(mnist['data'], k, mode='distance', p=10).todense()\n",
    "        labels = spectral.fit_predict(knn_graph)\n",
    "\n",
    "        # Visualise the clustering on PCA-projected dataset\n",
    "        plt.scatter(proj_mnist[:, 0], proj_mnist[:, 1], c=labels)\n",
    "        plt.title(\"Spectral Clustering: C = %d, k = %d\" % (C, k))\n",
    "        plt.show()\n",
    "\n",
    "        # Compute stability\n",
    "        mnist_stability[C].append(\n",
    "            cluster_stability(mnist['data'], \n",
    "                (lambda X, clusters: SpectralClustering(n_clusters=clusters, \n",
    "                                affinity=\"precomputed\").fit_predict(kneighbors_graph(X, k, mode='distance', p=10).todense())),\n",
    "                B=20, f=0.8))\n",
    "        \n",
    "    # Plot stability = f(k) for a given value of C\n",
    "    fs = plt.figure(C*len(n_neighbors_list))\n",
    "    plt.plot(n_neighbors_list, mnist_stability[C])\n",
    "    plt.title(\"Stability of Spectral Clustering as a function of the number of neighbors\\n Number of clusters: C = %d\" % C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summary of previous computations: plot of stability = f(k) for all values of C\n",
    "for C in n_clusters_list:\n",
    "    plt.plot(n_neighbors_list, mnist_stability[C], label=\"C = %d\" % C)\n",
    "plt.title(\"Stability of Spectral Clustering as a function of the number of neighbors\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.ylabel(\"Stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Learning\n",
    "\n",
    "Choice of the dataset used : **Precise** which dataset you chose and why it is relevant for the semi-supervised learning Task\n",
    "\n",
    "Advised datasets :\n",
    "\n",
    "*Breast Cancer Wisconsin (Diagnostic) Database*\n",
    "\n",
    "*MNIST binary even vs odd (multiple clusters inside each class)*\n",
    "\n",
    "Feel free to use other datasets if they are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "### For all the next questions, use Cancer and Mnist classes to handle your data if you choose to use these one,\n",
    "### You can also add more datasets but we advise you to handle them with this class for better readability\n",
    "class semi_sup_dat:\n",
    "    def __init__(self, data, p_unlabelled, name):\n",
    "        # DON T CHANGE THE RANDOM STATES\n",
    "        self.name = name\n",
    "        if self.name == 'Mnist':\n",
    "            # do an even vs odd binary classification :\n",
    "            even = [0, 2, 4, 6, 8]\n",
    "            Y = np.array([int(y in even) for y in data.target])\n",
    "        else:\n",
    "            Y = data.target\n",
    "        X_lab, X_unlab, y_lab, y_unlab = train_test_split(data.data, Y, test_size=p_unlabelled, random_state=32)\n",
    "        self.X_lab = X_lab\n",
    "        self.X_unlab = X_unlab\n",
    "        self.y_lab = y_lab\n",
    "        self.y_unlab = y_unlab\n",
    "\n",
    "\n",
    "# The following lines can be called later in the code to build a dataset with varying unlabelled proportion\n",
    "p_unlabelled = 0.8 # You can change this value\n",
    "cancer = load_breast_cancer()\n",
    "Cancer = semi_sup_dat(cancer, p_unlabelled, 'Cancer')\n",
    "\n",
    "digits = load_digits()\n",
    "Mnist = semi_sup_dat(digits, p_unlabelled, 'Mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Question 9  : Complete the function self_training\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def self_training_kNN(X, neighbors=3):\n",
    "    X_lab = X.X_lab.copy()\n",
    "    y_lab = X.y_lab.copy()    \n",
    "    X_unlab = X.X_unlab.copy()\n",
    "    y_unlab = X.y_unlab.copy() \n",
    "    \n",
    "    for i in range(len(y_unlab)):\n",
    "        kNN = KNeighborsClassifier(neighbors, n_jobs=-1)\n",
    "        kNN.fit(X_lab, y_lab)\n",
    "    \n",
    "        y_tmp = kNN.predict_proba(X_unlab)\n",
    "        \n",
    "        # Get the most probable prediction\n",
    "        # couple of indices: (index of sample, class)\n",
    "        idx, c = np.unravel_index(np.argmax(y_tmp), y_tmp.shape)\n",
    "        \n",
    "        X_lab = np.vstack([X_lab, X_unlab[idx, :]])\n",
    "        y_lab = np.hstack([y_lab, [c]])\n",
    "        \n",
    "        y_unlab[idx] = c\n",
    "        X_unlab = np.delete(X_unlab, idx, 0)\n",
    "        \n",
    "    return y_unlab\n",
    "\n",
    "# When the 2 classes are 0,1, this is the error\n",
    "def l1_loss (y_pred, data):\n",
    "    return np.linalg.norm(y_pred - data.y_unlab, 1) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test self-training on the cancer and mnist datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = self_training_kNN(Cancer, 3)\n",
    "print(100 * l1_loss(y_pred, Cancer))\n",
    "\n",
    "y_pred_mnist = self_training_kNN(Mnist, 3)\n",
    "print(100 * l1_loss(y_pred_mnist, Mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel # Or reimplement it yourself if your prefer\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 : Complete the code in the box below\n",
    "\n",
    "We aim at solving \n",
    "$$\\min_{f\\in \\mathcal{H}_\\mathcal{K}} \\frac{1}{l}\\sum_{i = 1}^l (y_i - f(x_i))^2 + \\lambda\\Vert f \\Vert_{\\mathcal{H}_\\mathcal{K}}^2 + \\frac{\\lambda_u}{u+l} f^\\top L f$$\n",
    "\n",
    "\n",
    "By applying the representer theorem, we get that there exits a vector $\\alpha \\in \\mathbb{R}^{l+u}$ such that the solution is of the form $f(.) = \\sum_{i=1}^{l+u} \\mathcal{K}(.,x_i)\\alpha_i$\n",
    "\n",
    "Let us denote $K$ the kernel matrix $\\lbrace\\mathcal{K}(x_i,x_j)\\rbrace_{i,j}$. Pluggin this expresssion into our minimization problem, we have that\n",
    "\n",
    "$\\begin{align}\n",
    "\\frac{1}{l}\\sum_{i = 1}^l (y_i - f(x_i))^2 &= \\frac{1}{l}\\sum_{i = 1}^l (y_i - \\sum_{j=1}^{l+u}\\mathcal{K}(x_i,x_j)\\alpha_j)^2\\\\\n",
    "&= \\frac{1}{l}\\Vert y \\Vert_2^2 - \\frac{2}{l}\\sum_{i=1}^{l}\\sum_{j=1}^{l+u}y_i\\alpha_j\\mathcal{K}(x_i,x_j) + \\frac{1}{l} \\sum_{j=1}^{l+u}\\sum_{l=1}^{l+u}\\alpha_j\\alpha_i\\mathcal{K}(x_i,x_j)\\\\\n",
    "&= \\frac{1}{l}\\Vert y \\Vert_2^2 - \\frac{2}{l} y^\\top K \\alpha + \\frac{1}{l}\\alpha^\\top K \\alpha\n",
    "\\end{align}$\n",
    "\n",
    "where $y \\in \\mathbb{R}^{l+u}, \\forall  j \\geq l, y_i = 0$ is the zero-padded vector of known labels.\n",
    "\n",
    "\n",
    "We also have that \n",
    "\n",
    "$\\begin{align}\n",
    "\\Vert f \\Vert_{\\mathcal{H}_\\mathcal{K}}^2 &=  \\left\\langle\\sum_{i=1}^{l+u} \\mathcal{K}(.,x_i)\\alpha_i, \\sum_{i=1}^{l+u} \\mathcal{K}(.,x_j)\\alpha_j \\right\\rangle_{\\mathcal{H}_\\mathcal{K}}\\\\\n",
    "&= \\sum_{i,j} \\alpha_i \\alpha_j \\mathcal{K}(x_i, x_j) \\text{ by the reproducing property of }\\mathcal{K(.,.)} \\\\\n",
    "&= \\alpha^\\top K \\alpha\n",
    "\\end{align}$\n",
    "\n",
    "and \n",
    "\n",
    "$\\begin{align}\n",
    "\\boldsymbol{f}^\\top L \\boldsymbol{f} &= \\sum_{ij} f(x_i) l_{ij} f_(x_j)\\\\\n",
    "&= \\sum_{ij}\\sum_{p,q}  \\alpha_p K_{ip}l_{ij} K_{jq} \\alpha_q\\\\\n",
    "&= \\alpha^\\top K^\\top L K\\alpha\n",
    "\\end{align}$\n",
    "\n",
    "Getting rid of the terms that do not play a role in the minimization, and taking the symmetry of $K$ into account, we can therefore rewrite our minimization problem as\n",
    "\n",
    "$$\\min_{\\alpha \\in \\mathbb{R}^{l+u}}  \\frac{1}{l}\\alpha^\\top K \\alpha  - \\frac{2}{l} y^\\top K \\alpha +  \\lambda\\alpha^\\top K \\alpha + \\frac{\\lambda_u}{u+l} \\alpha^\\top K L K\\alpha$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\\min_{\\alpha \\in \\mathbb{R}^{l+u}}  \\alpha^\\top \\left[\\left(\\frac{1}{l} + \\lambda \\right) K + \\frac{\\lambda_u}{u+l} K L K \\right] \\alpha  - \\frac{2}{l} y^\\top K \\alpha$$\n",
    "\n",
    "\n",
    "Since this is a convex minimization problem, at the optimum the gradient of the objective function vanishes, i.e. its solution $\\alpha^\\star$ verifies\n",
    "\n",
    "$$[(\\frac{1}{l} + \\lambda) K + \\frac{\\lambda_u}{u+l} K L K]\\alpha^\\star = \\frac{2}{l} K y$$\n",
    "\n",
    "which finally yields \n",
    "\n",
    "$$ \\alpha^\\star = \\frac{2}{l} [(\\frac{1}{l} + \\lambda) K + \\frac{\\lambda_u}{u+l} K L K]^{-1}K y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lapRLS(X, lbda, lbda_u):\n",
    "    l = X.y_lab.shape[0]\n",
    "    u = X.y_unlab.shape[0]\n",
    "    n = l + u\n",
    "    k = 3\n",
    "    \n",
    "    X_all = np.concatenate([X.X_lab, X.X_unlab])\n",
    "    kNN_graph = kneighbors_graph(X_all, k, mode='distance', p=10).todense()\n",
    "    \n",
    "    y = np.concatenate([X.y_lab, np.zeros(u)])\n",
    "    \n",
    "    K = rbf_kernel(X_all, X_all)\n",
    "    D = np.diag(np.sum(kNN_graph, axis=1))\n",
    "    L = D - K\n",
    "    \n",
    "    alpha = 2/l * np.linalg.inv((1/l + lbda)*K + (lbda_u)/n * K.dot(L).dot(K)).dot(K).dot(y)\n",
    "    \n",
    "    return lambda x: alpha.T.dot(rbf_kernel(X=X_all, Y=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lapRLS(Mnist, 0.1, 0.1)\n",
    "y_pred = f(Mnist.X_unlab)\n",
    "\n",
    "print(l1_loss(y_pred, Mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 : Add your answer here\n",
    "\n",
    "A major disadvantage of using a closed-form implementation is that we have to invert a $n\\times n$ matrix, where $n$ is the size of the dataset, which could potentially be non-invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_i(G,K,l,i,a,y):\n",
    "    return 2*G[i].dot(a) - 2./l*K[i].dot(y)\n",
    "\n",
    "def lapRLS_sgd(X, lbda, lbda_u, n_iter=100, step=1.):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    l = X.y_lab.shape[0]\n",
    "    u = X.y_unlab.shape[0]\n",
    "    n = l + u\n",
    "    k = 3\n",
    "    \n",
    "    iis = np.random.randint(0, n, n * n_iter) \n",
    "    \n",
    "    alpha = np.zeros(n)\n",
    "    \n",
    "    X_all = np.concatenate([X.X_lab, X.X_unlab])\n",
    "    kNN_graph = kneighbors_graph(X_all, k, mode='distance', p=10).todense()\n",
    "    \n",
    "    y = np.concatenate([X.y_lab, np.zeros(u)])\n",
    "    \n",
    "    K = rbf_kernel(X_all, X_all)\n",
    "    D = np.diag(np.sum(kNN_graph, axis=1))\n",
    "    L = D - K\n",
    "    \n",
    "    G = (1./l + lbda)*K + lbda_u/n* K.dot(L).dot(K)\n",
    "    \n",
    "    for idx in range(n_iter):\n",
    "        i = iis[idx]\n",
    "        \n",
    "        alpha = alpha - step / np.sqrt(idx+1) * grad_i(G,K,l,i, alpha,y)\n",
    "        \n",
    "    return lambda x: alpha.T.dot(rbf_kernel(X=X_all, Y=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = lapRLS_sgd(Mnist, 0.1, 0.1, n_iter = 100, step = 1.)\n",
    "y_pred = f(Mnist.X_unlab)\n",
    "\n",
    "print(l1_loss(y_pred, Mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 : Complete the code in the box below\n",
    "\n",
    "##### Add your answer to the question here :\n",
    "\n",
    "We aim at solving \n",
    "$$\\min_{f\\in \\mathcal{H}_\\mathcal{K}} \\frac{1}{l}\\sum_{i = 1}^l (1 - y_if(x_i))_+ + \\lambda\\Vert f \\Vert_{\\mathcal{H}_\\mathcal{K}}^2 + \\frac{\\lambda_u}{u+l} f^\\top L f$$\n",
    "\n",
    "\n",
    "By applying the representer theorem, we get that there exits a vector $\\alpha \\in \\mathbb{R}^{l+u}$ such that the solution is of the form $f(.) = \\sum_{i=1}^{l+u} \\mathcal{K}(.,x_i)\\alpha_i$\n",
    "\n",
    "As before, we have the following identities:\n",
    "\n",
    "$\\begin{align}\n",
    "\\Vert f \\Vert_{\\mathcal{H}_\\mathcal{K}}^2 = \\alpha^\\top K \\alpha\n",
    "\\end{align}$\n",
    "\n",
    "and \n",
    "\n",
    "$\\begin{align}\n",
    "\\boldsymbol{f}^\\top L \\boldsymbol{f} = \\alpha^\\top K^\\top L K\\alpha\n",
    "\\end{align}$\n",
    "\n",
    "\n",
    "Let us introduce slack variables $\\xi_i$ for $(1 - y_if(x_i))_+$\n",
    "\n",
    "Our problem then becomes \n",
    "\n",
    "$$\n",
    " \\left\\{\n",
    " \\begin{array}{lll}\n",
    " \\min_{\\alpha,\\xi} &\\frac{1}{l}\\sum_{i = 1}^l \\xi_i + \\lambda\\alpha^\\top K \\alpha + \\frac{\\lambda_u}{u+l} \\alpha^\\top K^\\top L K\\alpha\n",
    " \\\\\n",
    " \\mathrm{s.c.}&1 - y_i \\sum_{j=1}^{l+u}K_{ij}\\alpha_j\\geq \\xi_i\n",
    " \\\\\n",
    " \\mathrm{and}& \\xi_i \\geq 0\n",
    " \\end{array}\n",
    " \\right .\n",
    "$$\n",
    "\n",
    "The Lagrangian of this problem is \n",
    "\n",
    "$$\\mathcal{L}(\\alpha,\\xi,\\beta,\\mu) = \\frac{1}{l}\\sum_{i = 1}^l \\xi_i + \\lambda\\alpha^\\top K \\alpha + \\frac{\\lambda_u}{u+l} \\alpha^\\top K^\\top L K\\alpha - \\sum_{i=1}^l \\beta_i(y_i \\sum_{j=1}^{l+u}K_{ij}\\alpha_j - (1-  \\xi_i)) - \\sum_{i=1}^l \\mu_i\\xi_i$$\n",
    "\n",
    "Since the minimization problem is convex, the KKT conditions hold and at the optimum we have\n",
    "\n",
    "$\\nabla_\\xi \\mathcal{L}(\\alpha,\\xi,\\beta,\\mu) = 0$, i.e.\n",
    "\n",
    "$$\\frac{1}{l}\\mathbb{1}_l - \\beta - \\mu = 0 $$\n",
    "\n",
    "\n",
    "which implies that $0 \\leq \\beta \\leq \\frac{1}{l}\\mathbb{1}_l$, and allows us to get rid of the $\\xi_i$s (and $\\mu_i$s):\n",
    "\n",
    "$$\\mathcal{L}(\\alpha,\\beta) = \\lambda\\alpha^\\top K \\alpha + \\frac{\\lambda_u}{u+l} \\alpha^\\top K^\\top L K\\alpha - \\sum_{i=1}^l \\beta_i(y_i \\sum_{j=1}^{l+u}K_{ij}\\alpha_j -1)$$\n",
    "\n",
    "We also have that $\\nabla_\\alpha \\mathcal{L}(\\alpha,\\beta) = 0$, i.e.\n",
    "\n",
    "$$2(\\lambda K + \\frac{\\lambda_u}{u+l} K^\\top L K)\\alpha = K Y \\beta$$ \n",
    "\n",
    "where $Y = \\begin{pmatrix}\n",
    "\\mathrm{diag}(y) \\\\\n",
    "0 \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n\\times l}$,\n",
    "\n",
    "which allows us to express $\\alpha$ in terms of the dual variable $\\beta$:\n",
    "\n",
    "$$\\alpha = \\frac{1}{2}(\\lambda K + \\frac{\\lambda_u}{u+l} K^\\top L K)^{-1} K Y \\beta$$\n",
    "\n",
    "Therefore, the dual problem reads:\n",
    "\n",
    "$$\n",
    " \\left\\{\n",
    " \\begin{array}{lll}\n",
    " \\max_{\\beta}  &(K Y \\beta)^\\top\\frac{1}{2}(\\lambda K + \\frac{\\lambda_u}{u+l} K^\\top L K)^{-1}\\frac{1}{2} K Y \\beta - \\frac{1}{2}(K Y \\beta)^\\top(\\lambda K + \\frac{\\lambda_u}{u+l} K^\\top L K)^{-1} K Y \\beta + \\sum_{i=1}^l \\beta_i\n",
    " \\\\\n",
    " \\mathrm{s.c.}&0 \\leq \\beta \\leq \\frac{1}{l}\\mathbb{1}_l\n",
    " \\end{array}\n",
    " \\right .\n",
    "$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\n",
    " \\left\\{\n",
    " \\begin{array}{lll}\n",
    " \\min_{\\beta}  &\\frac{1}{2}\\beta^\\top Q \\beta - \\beta^\\top \\mathbb{1}_l\n",
    " \\\\\n",
    " \\mathrm{s.c.}& 0 \\leq \\beta \\leq \\frac{1}{l}\\mathbb{1}_l\n",
    " \\end{array}\n",
    " \\right .\n",
    "$$\n",
    "\n",
    "where $Q = \\frac{1}{2} (K Y)^\\top (\\lambda K + \\frac{\\lambda_u}{u+l} K^\\top L K)^{-1}K Y$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 : Complete the code in the box below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16 : Complete the code in the box below\n",
    "\n",
    "###### Describe your protocol here : \n",
    "-\n",
    "\n",
    "-\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
